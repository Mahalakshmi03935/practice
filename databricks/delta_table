# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType

# Initialize SparkSession
spark = SparkSession.builder.appName("Create Delta Lake Table") .getOrCreate()


schema = StructType([
    StructField("ID", StringType(), True),
    StructField("Name", StringType(), True),
    StructField("Age", IntegerType(), True),
    StructField("City", StringType(), True),
    StructField("Salary", FloatType(), True)
])


data = [
    ("1", "Raja", 30, "New York", 5000.0),
    ("2", "Maha", 25, "San Francisco", 6000.0),
    ("3", "Lakshmi", 35, "Los Angeles", 5500.0),
    ("4", "Bala", 28, "Chicago", 5200.0),
    ("5", "Saradha", 40, "Seattle", 5800.0)
]

# Create DataFrame
df = spark.createDataFrame(data, schema)

# Display DataFrame
df.show()

# Write DataFrame to Delta Lake
df.write.format("delta").saveasTable("default.delta_table")
